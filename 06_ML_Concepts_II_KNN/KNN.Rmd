---
title: "KNN"
author: "Brian Wright"
date: "October 31, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r}
library(caret)
library(tidyverse)
library(class)
library(plotly)
library(mice)
library(MLmetrics)
library(mltools)
library(data.table)
```


### What questions and business metric should we use? 
```{r}
bank_data <-  read_csv("data/bank.csv")

str(bank_data)
```

```{r}
(column_index <- tibble(colnames(bank_data)))

table(bank_data$marital)#three categories
table(bank_data$education)# four levels
table(bank_data$default) # two levels 
table(bank_data$job)# 12 levels...what we do? 
table(bank_data$contact)# 3 levels - but is there really any difference between cell and telephone
table(bank_data$housing) # two levels 
table(bank_data$poutcome)# four levels
table(bank_data$`signed up`)

bank_data$job <- fct_collapse(bank_data$job, 
                           Employed = c("admin.",
                                        "blue-collar",
                                        "entrepreneur",
                                        "housemaid",
                                        "management",
                                        "self-employed",
                                        "services",
                                        "technician"),
                           Unemployed= c("student",
                                         "unemployed",
                                         "unknown"))


```


### Convert the necessary columns to factors and check for missing data
```{r}
View(column_index)
bank_data[,c(2,3,4,5,8,7,13,14)] <- lapply(bank_data[,c(2,3,4,5,8,7,13,14)], as.factor)

str(bank_data)

#check for missing data

summary(bank_data)

md.pattern(bank_data)# wahooo

#Normalize the features that are numeric 
abc <- names(select_if(bank_data, is.numeric))

abc
bank_data[abc] <- lapply(bank_data[abc], normalize)

str(bank_data)
```

### Onehot encode

```{r}
ab_c <- names(select_if(bank_data, is.factor))#names collects the columns names, select_if selects columns based on a criteria in this example it's is.factor


bank_data <- one_hot(as.data.table(bank_data),cols=ab_c,sparsifyNAs = TRUE,naCols = FALSE,dropCols = TRUE,dropUnusedLevels = TRUE)

View(bank_data)

#Dropping signed up_0
bank_data <- bank_data[,-28]

str(bank_data)
```

### Data is ready so let's build the model
```{r}
# Let's run the kNN algorithm on our banking data. 
# Check the composition of labels in the data set. 
table(bank_data$`signed up_1`)[2]
table(bank_data$`signed up_1`)[2]/sum(table(bank_data$`signed up_1`))

# This means that at random, we have an 11.6% chance of correctly picking
# out a subscribed individual. Let's see if kNN can do any better.

part_index_1 <- createDataPartition(bank_data$`signed up_1`,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
View(part_index_1)

train <- bank_data[part_index_1,]
str(train)

tune_and_test <- bank_data[-part_index_1, ]


#The we need to use the function again to create the tuning set 

tune_and_test_index <- createDataPartition(tune_and_test$`signed up_1`,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]

str(train)
View(tune)
dim(test)

```
## Train the classifier 

```{r}
# Let's train the classifier for k = 9 using the class package. 

# k-Nearest Neighbor is a randomized algorithm, so make sure to
# use set.seed() to make your results repeatable.
set.seed(1984)
bank_9NN <-  knn(train = train,#<- training set cases
               test = tune,    #<- tune set cases
               cl = train$`signed up_1`,#<- category for true classification
               k = 6,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE)# provides the output in probabilities 

# View the output.
str(bank_9NN)
str(test)

table(bank_9NN)
table(tune$`signed up_1`)

bank_9NN

View(as.tibble(bank_9NN))
View(as.tibble(attr(bank_9NN,"prob")))

```

## Compare to the original data - Evaluation 

[Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)

```{r}
# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the 
# predictions from bank_3NN to the original data set.
kNN_res = table(bank_9NN,
                tune$`signed up_1`)
kNN_res
sum(kNN_res)  #<- the total is all the test examples

#Accuracy = TP+TN/(TP+TN+FP+FN)

(5791+717)/(5791+717+28+8)

# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]#essentially the left to right diagonal 

# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)

kNN_acc

# An 99.0% accuracy rate is pretty good but keep in mind the baserate is roughly 89/11, so we have more or less a 90% chance of guessing right if we don't know anything about the customer, but the negative outcomes we don't really care about, this models value is being able to id sign ups when they are actually sign ups. This requires us to know are true positive rate, or Sensitivity or Recall. So let's dig a little deeper.    

confusionMatrix(as.factor(bank_9NN), as.factor(tune$`signed up_1`), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

#sensitivity, recall and true poss rate = TP/TP+FN
#specificity, true negative rate = TN/TN+FP

#So our ability to "predict" sign up customers is at roughly 96% so that's  really solid. This means that out of 10 sign ups, classify 9ish correctly! Which is why in this case we would want to tune this model on TPR (Sensitivity), to get it has high as possible while sacrificing Specificity.  Similar to a medical diagnosis example, where we would rather produce false positives as compared to false negatives, predict more of those with cancer that don't have it as compared to missing anyone that actually has cancer.      

#Reference for confusion matrix: https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/confusionMatrix 


```

## Evaluation Examples 

### Selecting the correct "k"
```{r}
# How does "k" affect classification accuracy? Let's create a function
# to calculate classification accuracy based on the number of "k."
chooseK = function(k, train_set, val_set, train_class, val_class){
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}



# The sapply() function plugs in several values into our chooseK function.
#sapply(x, fun...) "fun" here is passing a function to our k-function
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop! Returns a matrix.
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                          train_set = train,
                          val_set = tune,
                          train_class = train$`signed up_1`,
                          val_class = tune$`signed up_1`))


View(knn_different_k)

#A bit more of a explanation...
seq(1,21, by=2)#just creates a series of numbers
sapply(seq(1, 21, by=2), function(x) x+1)#sapply returns a new vector using the series of numbers and some calculation that is repeated over the vector of numbers 

# Reformatting the results to graph
View(knn_different_k)
class(knn_different_k)#matrix 

knn_different_k = tibble(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

View(test)
View(knn_different_k)

# Plot accuracy vs. k.

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)

dev.off()

# 3 to 7 nearest neighbors seems to be a good choice because that's the
# greatest improvement in predictive accuracy before the incremental 
# improvement trails off.

```


### Adjusting the threshold
```{r}
str(bank_9NN)

bank_prob_1 <- tibble(attr(bank_9NN, "prob"))

View(bank_prob_1)

final_model <- tibble(k_prob=bank_prob_1$`attr(bank_9NN, "prob")`,pred=bank_9NN,target=tune$`signed up_1`)

View(final_model)

#Need to convert this to the likelihood to be in the poss class.
final_model$pos_prec <- ifelse(final_model$pred == 0, 1-final_model$k_prob, final_model$k_prob)

View(final_model)

#Needs to be a factor to be correctly  
final_model$target <- as.factor(final_model$target)

densityplot(final_model$pos_prec)

#confusionMatrix from Caret package
confusionMatrix(final_model$pred, final_model$target, positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

adjust_thres <- function(x, y, z) {
  #x=pred_probablities, y=threshold, z=tune_outcome
  thres <- as.factor(ifelse(x > y, 1,0))
  confusionMatrix(thres, z, positive = "1", dnn=c("Prediction", "Actual"), mode = "everything")
}

str(final_model)

adjust_thres(final_model$pos_prec,.30,as.factor(final_model$target))
#What's happening here? 

```


### More for next week :)
```{r}
library(ROCR)
pred <- prediction(final_model$pos_prec,final_model$target)
View(pred)

knn_perf <- performance(pred,"tpr","fpr")

plot(knn_perf, colorize=TRUE)
abline(a=0, b= 1)

knn_perf_AUC <- performance(pred,"auc")

print(knn_perf_AUC@y.values)

library(MLmetrics)
LogLoss(as.numeric(final_model$pos_prec), as.numeric(final_model$target))

F1_Score(y_pred = final_model$pred, y_true = final_model$target, positive = "1")

```

## Real quick - let's also look at a multi-class example using the iris dataset

```{r}

#For this example we are going to use the IRIS dataset in R
str(iris)
#first we want to scale the data so KNN will operate correctly
scalediris <- as.data.frame(scale(iris[1:4], center = TRUE, scale = TRUE)) 


str(scalediris)

set.seed(1000)
#We also need to create test and train data sets, we will do this slightly differently by using the sample function. The 2 says create 2 data sets essentially, replacement means we can reset the random sampling across each vector and the probability gives sample the weight of the splits, 2/3 for train, 1/3 for test. 
iris_sample <- sample(2, nrow(scalediris), replace=TRUE, prob=c(0.67, 0.33))
#We then just need to use the new variable to create the test/train outputs, selecting the first four rows as they are the numeric data in the iris data set and we want to predict Species (https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/sample)

#View(iris)

#View(iris_training)

#View(iris_sample)

iris_training <- scalediris[iris_sample==1, 1:4]
iris_test <- scalediris[iris_sample==2, 1:4]
#Now we need to create our 'Y' variables or labels need to input into the KNN function
iris.trainLabels <- iris[iris_sample==1, 5]
iris.testLabels <- iris[iris_sample==2, 5]
#So now we will deploy our model 
view(iris.trainLabels)

iris_pred <- knn(train = iris_training, test = iris_test, cl=iris.trainLabels, k=3, prob = TRUE)#probabilities are a percentage of points per class for each point, (kNN equals 4 for example and 3 of 4 are blue then 75% chance of being blue)

str(iris_pred)

xxxxx <- as.tibble(attr(iris_pred, "prob"))
View(xxxxx)


library(gmodels)
IRISPREDCross <- CrossTable(iris.testLabels, iris_pred, prop.chisq = FALSE)
#Looks like we got all but three correct, not bad


#You can also use caret for KNN, but it's not as specialized as the above, but but does have some additional capabilities for evaluation. 

```

## Example with Caret using 10-k cross-validation 

```{r}

set.seed(1981)
scalediris$Species <- iris$Species #adding back in the label for caret

iris_training_car <- scalediris[iris_sample==1, 1:5]  
iris_test_car <- scalediris[iris_sample==2, 1:5]

trctrl <- trainControl(method = "repeatedcv",
                       number = 10,
                       repeats = 3) # generic control to pass back into the knn mode using the cross validation method. 

iris_knn <- train(Species~.,
                  data = iris_training_car,
                  method="knn",
                  tuneLength=10,
                  trControl= trctrl,#cv method above, will select the optimal K
                  preProcess="scale") #already did this but helpful reference

iris_knn

plot(iris_knn)#can also plot

varImp(iris_knn)#gives us variable importance on a range of 0 to 100

iris_pred <- predict(iris_knn, iris_test_car)

iris_pred #gives a character predicted value for each row.

confusionMatrix(iris_pred, iris_test_car$Species)

table(iris_test_car$Species)#looks like we mis-classified 3 virginica as versicolor


xxxxx <- as.tibble(attr(iris_pred, "prob"))

View(xxxxx)

```



---
title: 'ML BOOTCAMP Dataset #1'
author: "Aishwarya Gavili"
date: "9/27/2021"
output:
  html_document:
    toc: TRUE
    theme: journal
    toc_float: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(psych)
library(mltools)
library(data.table)
library(caret)
library(plyr)
```

## Phase I
[NBA_Rookies_CareerLongevity_Dictionary](https://query.data.world/s/656zpxg6cfutn6ze5sxebmhcoxyby5)

```{r}
#Working to developed a model than can predict points per game based on if career length is >= 5 years or if career length is < 5 years. 

#Assuming we are able to optimizing and make recommendations how does this translate into a business context?

#The question I am trying to answer is whether shooting performance is correlated to the length of a basketball player's career.  Just because a player has been on a team for a while, does it mean they are still adding value with being consistently strong in their shooting performances? If not, is it better to cycle players, specifically rookies, more often?

# Inference versus Prediction 

# Independent Business Metric -   Assuming that longer NBA careers results in better shooting performance (points per game), can we predict shooting performance based on longevity of NBA career?

```

## Phase II 

### Scale/Center/Normalizing

```{r}
nba <- read.csv("https://query.data.world/s/6vjdw2ceugumrrc7ytuquerzoj4jux", header=TRUE, stringsAsFactors=FALSE)
#View(nba)
attach(nba)#is this a good idea? 
describe(nba)
?scale
str(nba)
(nba[,-c(1,2,21)] <- scale(nba[,-c(1,2,21)], center = TRUE, scale = TRUE)) #center and standardized 

(column_index <- tibble(colnames(nba)))

normalize <- function(x){
 (x - min(x)) / (max(x) - min(x))
}

# nba[,c(21)] <- lapply(nba[,c(21)], as.factor)
# nba$TARGET_5Yrs <- ifelse(nba$TARGET_5Yrs == 1 , "yes", "no")
# nba$TARGET_5Yrs<-as.factor(nba$TARGET_5Yrs)
# str(nba)

abc <- names(select_if(nba, is.numeric))# select function to find the numeric variables 

nba[abc] <- as_tibble(lapply(nba[abc], normalize))

#drop na columns
nba <- nba[,-c(10)]

str(nba)
describe(nba)

```

### One-hot Encoding 
[ML Tools One-Hot Overview](https://www.rdocumentation.org/packages/mltools/versions/0.3.5/topics/one_hot)

```{r}
# Next let's one-hot encode those factor variables/character 

?one_hot

nba_1h <- one_hot(as.data.table(nba),cols = "auto",sparsifyNAs = TRUE,naCols = TRUE,dropCols = TRUE,dropUnusedLevels = TRUE)
#View(nba_1h)
```

### Baseline/Prevalance 

```{r}
#Essential the target to which we are trying to better with our model. 
describe(nba_1h$PTS)
(box <- boxplot(nba_1h$PTS, horizontal = TRUE)) 
box$stats
fivenum(nba_1h$PTS)
 ?fivenum
# 
# #added this a predictor versus replacing the numeric version
(nba_1h$PTS_f <- cut(nba_1h$PTS,c(-1,0.1763636,1),labels = c(0,1)))
# 
str(nba_1h)
nba_1h
# #So no let's check the prevalence 
(prevalence <- table(nba_1h$PTS_f )[[2]]/length(nba_1h$PTS_f ))

```
### Initial Model Building: Decision Tree Style  

```{r}
# Training, Evaluation, Tune, Evaluation, Test, Evaluation
# Divide up our data into three parts, Training, Tuning, and Test

#There is not a easy way to create 3 partitions using the createDataPartitions

#so we are going to use it twice. Mostly because we want to stratify on the variable we are working to predict. What does that mean?  

#clean up our dataset a bit by dropping the original ranking variable and the nba name which we can't really use. 

nba_dt <- nba_1h[,-c("Name","PTS")]
# view(nba_dt)
nba_dt
part_index_1 <- caret::createDataPartition(nba_dt$PTS_f,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
# View(part_index_1)
dim(nba_dt)

train <- nba_dt[part_index_1,]
tune_and_test <- nba_dt[-part_index_1, ]

#The we need to use the function again to create the tuning set 

tune_and_test_index <- createDataPartition(tune_and_test$PTS_f,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]


dim(train)
dim(tune)
dim(test)


```

#### Using Caret package to fit a C5.0 version of a decision tree
Setting up the cross validation
[Caret_Documentation](http://topepo.github.io/caret/train-models-by-tag.html#Tree_Based_Model)
```{r}
#Cross validation process 

fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5, 
                          returnResamp="all") 

# number - number of folds
# repeats - number of times the CV is repeated, here it's 5 take the average of
# those 5 repeats


# Choose the features and classes

```


#### Training and Evaluation 

```{r}
features <- train[,-"PTS_f"]
target <- train[,"PTS_f"]

str(target)

set.seed(1984)
nba_mdl <- train(x=features,
                y=target$PTS_f,
                trControl=fitControl,
                method="C5.0",
                verbose=TRUE)

nba_mdl

```

Tune and Evaluation
```{r}
nba_predict = predict(nba_mdl,tune,type= "raw")

confusionMatrix(as.factor(nba_predict), 
                as.factor(tune$PTS_f), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")

varImp(nba_mdl)

plot(nba_mdl)


grid <- expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(20,30,40), 
                    .model=c("tree","rules"))
set.seed(1984)
nba_mdl_tune <- train(x=features,
                y=target$PTS_f,
                tuneGrid=grid,
                trControl=fitControl,
                method="C5.0",
                verbose=TRUE)

nba_mdl_tune
nba_mdl

plot(nba_mdl_tune)

# Want to evaluation again with the tune data using the new model 

nba_predict_tune = predict(nba_mdl_tune,tune,type= "raw")

confusionMatrix(as.factor(nba_predict_tune), 
                as.factor(tune$PTS_f), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")



```

### Test
```{r}
nba_predict_test = predict(nba_mdl_tune,test,type= "raw")

confusionMatrix(as.factor(nba_predict_test), 
                as.factor(test$PTS_f), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")

```


#### Summary/Findings

Through the analysis above, my goal was to find out if players in the NBA with longer careers had better performance.  More specifically, I wanted to see whether rookies or players with more experience were adding the most value to the team.  And consequently, allowing sports leagues like the NBA to decide whether they want to cycle in rookies/newbies and cycle out old players more frequently.  For this reason I made my target variable points per game (PTS).  However after tuning and evaluating my data, I found that the longevity of career (ie. greater or less than 5 years) was the feature with the least importance in predicting performance or points per game.  Instead, field goal attempts, field goals made, and free throw attempts, that were the most important features in predicting shooting performance. Some issues I encountered in this analysis process included system errors with loading certain packages (fixed by loading binary versions) and figuring out how to tune the hyper parameters as I am not too familiar with the C50 model.


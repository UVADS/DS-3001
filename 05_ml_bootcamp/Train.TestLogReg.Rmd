---
title: "Train.TestLogReg.Rmd"
author: "Brian Wright"
date: "March 24, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(pscl)
library(ResourceSelection)
library(pROC)
library(ROCR)
library(ISLR)
```

```{r}
data <- read_csv("~/git_3001/DS-3001/data/LogReg.csv")
head(data)
str(data)
plot(data$admit)
data$rank <- factor(data$rank)
data$admit <- factor(data$admit)
admitlogit <- glm(admit~gre+gpa+rank, family = binomial(link = "logit"), data = data)
summary(admitlogit)
#Because the output is in log terms we need to convert to % using exp
exp(coef(admitlogit))

#Test our models goodness of fit 
hoslem.test(data$admit, fitted(admitlogit))
#We can also use a ROC curve gauge how well we are classifying the data in combination with the predict function

#First we want to use the data to predict the likelihood that each student will be admitted, similar to residual deviance  
prob=predict(admitlogit, type = c("response"))
prob

#Then we can plot our hit "rate hit"
h <- roc(admit~prob, data=data)
plot(h)

```
Try on mtcars, developed ROC, convert the coef to %

```{r}
head(mtcars)
str(mtcars)
carsmodel <- glm(formula = vs~wt+disp, family = binomial(link = "logit"), data = mtcars)
summary(carsmodel)
hoslem.test(mtcars$vs, fitted(carsmodel))
exp(coef(carsmodel))

```

Below is a example of how we can split the dataset into test and train
```{r}

#Data set we will be working with is...default 
attach(Default)
head(Default)
str(Default)
table(Default$default)
tail(Default, 100)
str(Default)
str(default)
#So let's try to predict who will default by using a more common machine learning process of splitting the data into train and test segmenets. We are going to to go with a 80 train 20% test appoarch. Not that the common and space tells R to include the entire data.frame
train <- Default[1:8000, ]
test <- Default[8001:10000, ]
#Next we will run our logisitc regression model, I'm adding the logit phrase but binomial does this by default so it is not necessary. 
default.model <- glm(default~.,family="binomial"(link = "logit"),train)

summary(default.model)

#We drop income
default.model2 <- glm(default~student+balance,family="binomial"(link = "logit"),train)
summary(default.model2)

#Just out of curiousty
default.model3 <- glm(default~student+income,family="binomial"(link = "logit"),train)
summary(default.model3)

#Ok income still stinks and our AIC went way up back to the other model, so we will stick with model 2
#The output is express in log odds, but we can convert to % via exp() function
model2.ouput <- exp(coef(default.model2))
model2.ouput
```

We can also evaluate our varaible choices using the ANOVA function and the likelihood ratio test to see if significant changes have occured.  
```{r}
anova(default.model, test="LRT")
#again there is no significant change in residual deviation when we add income, so see ya
```

Moving on, we want to do some predicting and evaluate our model further
```{r}
pred.model2 <- predict.glm(default.model2,test,type='response')
#iselse works like so...ifelse(test, yes,no), so we are classifying the output if greater than .05 label as 1 if not lable as a 0
pred.model2
head(pred.model2)
contrasts(default)
pred.model2.1 <- ifelse(pred.model2 > 0.5,1,0)
#essentially we are creating percentage likelihood of default for each value, above 50% we are saying it's more likely to occur. 
head(pred.model2.1)
model3hit <- mean(pred.model2.1!=test$default)
model3hit
#Wow we are amazing! But let's double check to make sure we are as awesome as we think, let's check something. 
#Or are we really bad...because usually perfect is a sign of error...any guesses?

test %>% group_by(default) %>%
  summarise(no_rows = length(default))
#So we only had to correctly identify 67 Yes factors, which is a pretty small percentage, but still enough to where we should be getting some level of error...

#What type of variable is default?
head(test$default)
model3hit.1 <- mean(pred.model2.1!=as.numeric(test$default))
model3hit.1


```
For our next trick let's do a roc curve, just to confirm our model, using a different package then our first example
```{r}

#In order to use the package we first have to set the prediction 
newpred <- prediction(pred.model2,test$default)
#Next we want to measure true possitives which is "tpr" and also False Positives "fpr"
newpred.performance <- performance(newpred, measure = "tpr",x.measure = "fpr")
#then we plot these two measures
plot(newpred.performance)
#Looking pretty good, we can also get the AUC again using the performance function
AUC <- performance(newpred, measure = "auc")
AUC
```


Finally we want to actually predict something right?!
```{r}
xx <- data.frame(student="Yes",balance=5000)
xxpred <- predict(default.model2,xx,type='response')
xxpred
#Pretty high chance according to the data we have, what if we cut the balance to 2000?
xx.11 <- data.frame(student="Yes",balance=2000)
xxpred.1 <- predict(default.model2,xx.11,type='response')
xxpred.1
#Likelihood drops to 52%
new.data.test <- data.frame(student="Yes",balance=1:10000)
new.data.test.pred <- data.frame(predict(default.model2,new.data.test, type = "response"))
head(new.data.test.pred)
scatter.smooth(new.data.test.pred, xlab = "Credit Card Debt", ylab = "Likelihood of Default")
```

In class, let's use the email dataset in the "openintro" package to try to predict spam, start by using to_multiple, cc, attach, dollar, winner, inherit, viagra password, format, re-subj and exlaim-subj, in the final model only use variables that are contributing. Divide the data set into test and train prior to model reduction. Lastly use your model to predict spam on the data then assess how well you did by generating "hit rate" and a ROC curve. How good is your model?

```{r}
library(openintro)
str(email)


```




---
title: "Clustering Lab"
author: "Adriel Kim"
date: "10/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Goal: Know how to make decisions and answer questions using clustering. 

Repeat the clustering process only using the Rep house votes dataset
- What differences and similarities did you see between how the clustering 
worked for the datasets?

```{r}

# Load libraries.
#library(e1071)
library(tidyverse)
library(plotly)
library(htmltools)
library(devtools)
library(caret)
library(NbClust)


```


```{r}
#Select the variables to be included in the cluster 
house_votes_Rep = read_csv("../data/house_votes_Rep.csv")

table(house_votes_Rep$party.labels)
View(house_votes_Rep)


```

```{r}
clust_data_Rep = house_votes_Rep[, c("aye", "nay", "other")]
View(clust_data_Rep)


set.seed(1)
kmeans_obj_Rep = kmeans(clust_data_Rep, centers = 2, 
                        algorithm = "Lloyd")  

kmeans_obj_Rep

# View the results of each output of the kmeans function.
head(kmeans_obj_Rep)

```

```{r}
#Visualize the output

party_clusters_Rep = as.factor(kmeans_obj_Rep$cluster)

# What does the kmeans_obj look like?
View(party_clusters_Rep)

View(party_clusters_Rep)

ggplot(house_votes_Rep, aes(x = aye, 
                            y = nay,
                            shape = party_clusters_Rep)) + 
  geom_point(size = 6) +
  ggtitle("Aye vs. Nay votes for Republican-introduced bills") +
  xlab("Number of Aye Votes") +
  ylab("Number of Nay Votes") +
  scale_shape_manual(name = "Cluster", 
                     labels = c("Cluster 1", "Cluster 2"),
                     values = c("1", "2")) +
  theme_light()
```
```{r}
ggplot(house_votes_Rep, aes(x = aye, 
                            y = nay,
                            color = party.labels,  #<- tell R how to color 
                            #   the data points
                            shape = party_clusters_Rep)) + 
  geom_point(size = 4) +
  ggtitle("Aye vs. Nay votes for Republican-introduced bills") +
  xlab("Number of Aye Votes") +
  ylab("Number of Nay Votes") +
  scale_shape_manual(name = "Cluster", 
                     labels = c("Cluster 1", "Cluster 2"),
                     values = c("1", "2")) +
  scale_color_manual(name = "Party",         #<- tell R which colors to use and
                     #   which labels to include in the legend
                     labels = c("Republican", "Repocratic"),
                     values = c("red", "blue")) +
  theme_light()



```

```{r}
#Evaluate the quality of the clustering 
# Inter-cluster variance,
num_Rep = kmeans_obj_Rep$betweenss

# Total variance, "totss" is the sum of the distances
# between all the points in the data set.
denom_Rep = kmeans_obj_Rep$totss

# Variance accounted for by clusters.
(var_exp_Rep = num_Rep / denom_Rep)

```

```{r}
#Use the function we created to evaluate several different number of clusters

# Run an algorithm with 3 centers.
set.seed(1)
kmeans_obj_Rep = kmeans(clust_data_Rep, centers = 3, algorithm = "Lloyd")

# Inter-cluster variance.
num_Rep3 = kmeans_obj_Rep$betweenss

# Total variance.
denom_Rep3 = kmeans_obj_Rep$totss

# Variance accounted for by clusters.
(var_exp_Rep3 = num_Rep3 / denom_Rep3)

#Might be a helpful look to compare to just a normal variance calculation:
# s2=âni=1(xiâ xÂ¯)2/(nâ1) = variance equation for var()

total.var <- var(clust_data_Rep$aye)+var(clust_data_Rep$nay)+var(clust_data_Rep$other)

total.var.km <- (kmeans_obj_Rep$betweenss+kmeans_obj_Rep$tot.withinss)/(427-1)

# Numbers are the same. 
total.var
total.var.km

```
```{r}
# The function explained_variance wraps our code for calculating 
# the variance explained by clustering.
explained_variance = function(data_in, k){
  
  # Running the kmeans algorithm.
  set.seed(1)
  kmeans_obj = kmeans(data_in, centers = k, algorithm = "Lloyd", iter.max = 30)
  
  # Variance accounted for by clusters:
  # var_exp = intercluster variance / total variance
  var_exp = kmeans_obj$betweenss / kmeans_obj$totss
  var_exp  
}
```

```{r}
#Create a elbow chart of the output
# Recall the variable we are using for the data that we're clustering.
View(clust_data_Rep)

# The sapply() function plugs in several values into our explained_variance function.
#sapply() takes a vector, lapply() takes a dataframe
explained_var_Rep = sapply(1:10, explained_variance, data_in = clust_data_Rep)

View(explained_var_Rep)


# Data for ggplot2.
elbow_data_Rep = data.frame(k = 1:10, explained_var_Rep)
View(elbow_data_Rep)

# Plotting data.
ggplot(elbow_data_Rep, 
       aes(x = k,  
           y = explained_var_Rep)) + 
  geom_point(size = 4) +           #<- sets the size of the data points
  geom_line(size = 1) +            #<- sets the thickness of the line
  xlab('k') + 
  ylab('Inter-cluster Variance / Total Variance') + 
  theme_light()
```

```{r}
#Use NbClust to select a number of clusters
library(NbClust)

# Run NbClust.
(nbclust_obj_Rep = NbClust(data = clust_data_Rep, method = "kmeans"))

# View the output of NbClust.
nbclust_obj_Rep

# View the output that shows the number of clusters each method recommends.
View(nbclust_obj_Rep$Best.nc)

```

```{r}
#Display the results visually 
freq_k_Rep = nbclust_obj_Rep$Best.nc[1,]
freq_k_Rep = data.frame(freq_k_Rep)
View(freq_k_Rep)

# Check the maximum number of clusters suggested.
max(freq_k_Rep)

#essentially resets the plot viewer back to default
#dev.off()

# Plot as a histogram.
ggplot(freq_k_Rep,
       aes(x = freq_k_Rep)) +
  geom_bar() +
  scale_x_continuous(breaks = seq(0, 15, by = 1)) +
  scale_y_continuous(breaks = seq(0, 12, by = 1)) +
  labs(x = "Number of Clusters",
       y = "Number of Votes",
       title = "Cluster Analysis")
```

```{r}
#Using the recommended number of cluster compare the output to the elbow chart method, assuming it's different. 

#Both methods recommend the same number of clusters. The cluster analysis above recommends k=2 is
#best since it has the highest number of votes. The elbow chart also recommends k=2 which we can see
#at the "elbow" of the chart.
```

```{r}
# What differences and similarities did you see between how the clustering 
# worked for the datasets? What do these patterns suggest about the           
# differences between republican versus  

#The republican dataset seem to form more distinct clusters. There was less overlap between the
#democratic and republican parties. This suggests that republican introduced bills are most likely related
#to issues on which democratics and republicans are sharply divided on.
```



  






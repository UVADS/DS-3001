---
title: "Week11_ReinforcementLab"
author: "SarahGould"
date: "11/3/2021"
output: html_document
---

#### Installing Packages + Libraries

```{r, include=FALSE}
library(tidyverse)
library(plotly)
library(htmltools)
library(devtools)
library(caret)
library(NbClust)
```

### Data Prep + Analysis

#### Initial Data Cleaning + Plot

```{r, mesage=FALSE}
# read in data and create dataframe (df1)
df <- read_csv("~/Documents/UVA/DS 3001/DS-3001-main/week-11-reinforcement-lab/data-summary.csv")
df1 <- select(df,main_colors,opp_colors,on_play,num_turns,won)

# feature engineering (cora,corc)
df2 <- select(df,"deck_Adeline, Resplendent Cathar":"deck_Wrenn and Seven")
mat = data.matrix(df2)
vec1 <- vector()
vec3 <- vector()
for(i in 1:nrow(mat) ){
  x<-cor( mat[1,] , mat[i,])
  vec1 <- c(vec1,x)
  z<-cor( mat[47,] , mat[i,])
  vec3 <- c(vec3,z)
}

# add new features to dataframe
df1 <- df1 %>% mutate(cora = vec1)
df1 <- df1 %>% mutate(corc = vec3)

# make scatter plot comparing new features
ggplot(df1,aes(x=cora,y=corc))+geom_point()
```

#### Improvements to Data

```{r}
# Cluster the Continuous Variable
clust_df1 = df1[, c("num_turns","cora", "corc")]
clust_df1 <- na.omit(clust_df1)

# Normalize Data
normalize <- function(x){
 (x - min(x)) / (max(x) - min(x))
}
numCol <- names(select_if(df1, is.numeric))
numCol <- numCol[1:3]
clust_df1[numCol] <- as_tibble(lapply(clust_df1[numCol], normalize))

# Use kmeans
set.seed(1) # randomly set the seed to randomly cluster objects
kmeans_obj_df1 = kmeans(clust_df1, centers = 4, 
                        algorithm = "Lloyd")
head(kmeans_obj_df1)
clusters_df1 = as.factor(kmeans_obj_df1$cluster)
```

#### NbClust

```{r}
# use NbClust to select a number of clusters
nbclust_obj_df1 = NbClust(data = clust_df1, method = "kmeans")

freq_k_df1 = nbclust_obj_df1$Best.nc[1,]
freq_k_df1 = data.frame(freq_k_df1)

# Check the maximum number of clusters suggested.
max(freq_k_df1)

# Plot as a histogram. # 4 WON !! :))
ggplot(freq_k_df1,
       aes(x = freq_k_df1)) +
  geom_bar() +
  scale_x_continuous(breaks = seq(0, 15, by = 1)) +
  scale_y_continuous(breaks = seq(0, 12, by = 1)) +
  labs(x = "Number of Clusters",
       y = "Number of Votes",
       title = "Cluster Analysis")
```

#### Clustering via ggplot

```{r}
# Plot with Clusters + Colors
ggplot(df1, aes(x = cora, 
                            y = corc,
                            color = clusters_df1)) + 
    # shape: defined by the clusters
  geom_point(size = 3) +
  ggtitle("cora vs. corc") +
  xlab("corc") +
  ylab("cora") +
  # scale_shape_manual(name = "Cluster", 
  #                    labels = c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4"),
  #                    values = c("1", "2", "3", "4")) +
  scale_color_manual(name = "Cluster",
                     labels = c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4"),
                     values = c("pink", "orange", "red", "yellow")) +
  theme_light()

ggsave("week11_cluster.png", 
       width = 10, 
       height = 5.62, 
       units = "in")

```


#### Reflection

After analyzing the data and performing necessary improvements, I found that clustering was the most useful form of visualizing the data. Clustering by kmeans is a method that partitions data into a particular number of n clusters that clusters similar data together to aid in analysis and visualizations. Adding the color by cluster and adjusting the size of the points, were also a helpful visualization tool. The final ggplot graph illustrated that the clustering performed, and the number of clusters designated, was the most suitable for this particular dataset. 
To determine the necessary number of clusters, I performed NbClust and plotted  a bar graph to visualize the best number of clusters. I found that 3 and 4 were the best for this graph, so I personally chose 4 clusters during my future kmeans calculations.

My recommendations for future datasets includes:

1. Clean intial dataset and normalize
2. Find the best number of clusters via NbClust
3. Cluster the data
4. Plot with added layers for color and size to further visualize data and trends

